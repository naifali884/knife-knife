{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw2xbx8TXSh9",
        "outputId": "b4a8dee8-a3e1-45e6-ade6-a951e5cdfbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Id             150 non-null    int64  \n",
            " 1   SepalLengthCm  150 non-null    float64\n",
            " 2   SepalWidthCm   150 non-null    float64\n",
            " 3   PetalLengthCm  150 non-null    float64\n",
            " 4   PetalWidthCm   150 non-null    float64\n",
            " 5   Species        150 non-null    object \n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 7.2+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming 'Iris.csv' is in the same directory as the script\n",
        "# If not, replace with the correct path\n",
        "iris_path = '/content/Iris.csv'  # Or the full path if it's elsewhere\n",
        "\n",
        "# Load the Iris dataset\n",
        "try:\n",
        "    iris_df = pd.read_csv(iris_path)\n",
        "    # Display basic information and a preview of the dataset\n",
        "    iris_info = iris_df.info()\n",
        "    iris_head = iris_df.head()\n",
        "    iris_info, iris_head\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {iris_path}. Please check the file path.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(iris_info)\n",
        "print(iris_head)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL6pwe05YKoh",
        "outputId": "5aeaf239-7be1-4ba6-edf7-50ec51816b2e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
            "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
            "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
            "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
            "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
            "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "Drop non-informative columns like Id.\n",
        "Encode categorical variables (the Species column) if not numeric.\n",
        "Scale the features using standardization for better performance in clustering and classification."
      ],
      "metadata": {
        "id": "Szuq4pnXZBZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "iris_df = pd.read_csv('/content/Iris.csv')\n",
        "\n",
        "# Drop Id column\n",
        "iris_df = iris_df.drop(columns=['Id'])\n",
        "\n",
        "# Encode target column 'Species'\n",
        "label_encoder = LabelEncoder()\n",
        "iris_df['Species'] = label_encoder.fit_transform(iris_df['Species'])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = iris_df.drop(columns=['Species'])\n",
        "y = iris_df['Species']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "Tob45ttHZNnF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print labels dictionary\n",
        "labels_dict = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
        "print(\"Labels dictionary:\", labels_dict)\n"
      ],
      "metadata": {
        "id": "x7b7QKH6csHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7df4b88-69f6-4d80-d773-f62f66dbb8c9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels dictionary: {0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply Unsupervised Learning Techniques (Clustering)**\n",
        "Use K-means and Hierarchical Clustering. Then validate using Silhouette Score and Davies-Bouldin Index."
      ],
      "metadata": {
        "id": "eNQS6JfjZaOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# K-means Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Hierarchical Clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=3)\n",
        "hierarchical_labels = hierarchical.fit_predict(X_scaled)\n",
        "\n",
        "# Validation metrics\n",
        "kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
        "kmeans_davies_bouldin = davies_bouldin_score(X_scaled, kmeans_labels)\n",
        "hierarchical_silhouette = silhouette_score(X_scaled, hierarchical_labels)\n",
        "hierarchical_davies_bouldin = davies_bouldin_score(X_scaled, hierarchical_labels)\n",
        "\n",
        "print(\"KMeans Silhouette Score:\", kmeans_silhouette)\n",
        "print(\"KMeans Davies-Bouldin Index:\", kmeans_davies_bouldin)\n",
        "print(\"Hierarchical Silhouette Score:\", hierarchical_silhouette)\n",
        "print(\"Hierarchical Davies-Bouldin Index:\", hierarchical_davies_bouldin)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quACoC6kZkHr",
        "outputId": "4843e01a-967c-4078-90c4-6b55813d1969"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans Silhouette Score: 0.4787241921049546\n",
            "KMeans Davies-Bouldin Index: 0.7868006762339902\n",
            "Hierarchical Silhouette Score: 0.44553956399200406\n",
            "Hierarchical Davies-Bouldin Index: 0.805940469032997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**\n",
        "You can use Variance Threshold or Recursive Feature Elimination (RFE) to select important features."
      ],
      "metadata": {
        "id": "nRxT826QZ-ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Apply Variance Threshold to remove low-variance features\n",
        "selector = VarianceThreshold(threshold=0.1)\n",
        "X_selected = selector.fit_transform(X_scaled)\n"
      ],
      "metadata": {
        "id": "BwoeM4tEaBjJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply Supervised Learning Classifiers**\n",
        "Choose five classifiers, potentially using boosting algorithms.\n",
        "\"Logistic Regression\"\n",
        "\"Random Forest\"\n",
        "\"Gradient Boosting\"\n",
        "\"AdaBoost\"\n",
        "\"K-Nearest Neighbors\""
      ],
      "metadata": {
        "id": "lhXW2E1ybij2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = accuracy\n",
        "    print(f\"Results for {name}:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Select the best model based on accuracy\n",
        "best_model_name = max(results, key=results.get)\n",
        "best_model = classifiers[best_model_name]\n",
        "print(f\"Best model: {best_model_name} with accuracy {results[best_model_name]:.2f}\")\n",
        "\n",
        "# Save the best model to a file\n",
        "joblib.dump(best_model, 'model.pkl')\n",
        "print(\"Model saved as model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6a9B3tGb5Xs",
        "outputId": "91132a41-70e0-423f-fd16-917f9d86fecb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Logistic Regression:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "\n",
            "\n",
            "Results for Random Forest:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "\n",
            "\n",
            "Results for Gradient Boosting:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "\n",
            "\n",
            "Results for AdaBoost:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "\n",
            "\n",
            "Results for K-Nearest Neighbors:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "\n",
            "\n",
            "Best model: Logistic Regression with accuracy 1.00\n",
            "Model saved as model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Titanic Survival Datasets  **                                  \n",
        "Principal Component Analysis (PCA):                                                                          Import PCA:"
      ],
      "metadata": {
        "id": "kMTPZitnyRFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import PCA:"
      ],
      "metadata": {
        "id": "IOYmsJ4mzGfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WJ1CqlU_zgRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "hbKosGc7yMK5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Fit PCA:\n",
        "\n",
        "Retain components that explain about 95% of the variance:"
      ],
      "metadata": {
        "id": "yACLYQCqzcTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Titanic dataset - replace with your actual file path\n",
        "titanic_data = pd.read_csv('/content/titanic.csv')  # Replace with the correct path\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X = titanic_data.drop(columns=['Survived'])\n",
        "y = titanic_data['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KYyO_ttfzh87"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify non numeric columns\n",
        "print(X_train.dtypes)\n"
      ],
      "metadata": {
        "id": "o1ulPjzB0zYJ",
        "outputId": "596cfec4-454b-40a6-c3c9-eff08f6a9f87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId      int64\n",
            "Pclass           int64\n",
            "Name            object\n",
            "Sex             object\n",
            "Age            float64\n",
            "SibSp            int64\n",
            "Parch            int64\n",
            "Ticket          object\n",
            "Fare           float64\n",
            "Cabin           object\n",
            "Embarked        object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Non-Numeric Columns\n",
        "X = X.drop(columns=['Cabin'], errors='ignore')\n"
      ],
      "metadata": {
        "id": "wXY-eo_n1yPa"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reprocess missing values for numeric features only\n",
        "numeric_features = X.select_dtypes(include=['number']).columns\n",
        "X[numeric_features] = X[numeric_features].fillna(X[numeric_features].median())"
      ],
      "metadata": {
        "id": "PY5Ml16h2kJk"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Proceed with Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "YnTys4PS2tCK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaned Preprocessing Code:\n",
        "# Drop irrelevant columns and handle missing values\n",
        "X = titanic_data.drop(columns=['Survived', 'Name', 'Ticket', 'Cabin'], errors='ignore')\n",
        "X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
        "X['Embarked'] = X['Embarked'].map({'C': 1, 'Q': 2, 'S': 3})\n",
        "\n",
        "# Fill missing values with median\n",
        "X.fillna(X.median(), inplace=True)\n",
        "\n",
        "# Split the data into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ACHiiwyU3CZr"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n"
      ],
      "metadata": {
        "id": "DyKpA3Sz3s66"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "FiF7gGxH3uch"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Logistic Regression\n",
        "logreg_pca = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predictions and Accuracy\n",
        "y_pred_pca_logreg = logreg_pca.predict(X_test_pca)\n",
        "accuracy_pca_logreg = accuracy_score(y_test, y_pred_pca_logreg)\n",
        "\n",
        "print(\"Logistic Regression with PCA Accuracy:\", accuracy_pca_logreg)\n"
      ],
      "metadata": {
        "id": "W1WX0h_U3-6I",
        "outputId": "fa213da3-b9da-453d-d911-535ac1b1cd13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with PCA Accuracy: 0.6071428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression with PCA Accuracy is 60.71%, which is significantly lower than the accuracy achieved with manual feature engineering (100%).     "
      ],
      "metadata": {
        "id": "S-Xkv2ZR4R3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Feature Elimination (RFE)\n",
        "Let's apply Recursive Feature Elimination (RFE) to see if it can identify a subset of features that improves accuracy. I will now proceed to implement RFE."
      ],
      "metadata": {
        "id": "Ff9xnEjE4dZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize logistic regression as the base estimator for RFE\n",
        "rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42), n_features_to_select=5)\n",
        "\n",
        "# Fit RFE on the training data\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on RFE-selected features\n",
        "logreg_rfe = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg_rfe.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Predictions and Accuracy\n",
        "y_pred_rfe_logreg = logreg_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe_logreg = accuracy_score(y_test, y_pred_rfe_logreg)\n",
        "\n",
        "accuracy_rfe_logreg\n"
      ],
      "metadata": {
        "id": "9P5bZkaV4Tdr",
        "outputId": "46134ed7-0fab-4ce3-b2a4-625b168f63fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Feature Elimination (RFE)"
      ],
      "metadata": {
        "id": "0_EmBPYe42pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n"
      ],
      "metadata": {
        "id": "4BkFX0L746G4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42), n_features_to_select=5)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n"
      ],
      "metadata": {
        "id": "9mc9Wcr-48QL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are already defined (your features and target variables)\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply RFE with Logistic Regression as estimator to rank features\n",
        "rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42), n_features_to_select=5)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train and evaluate Logistic Regression\n",
        "logreg_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe_logreg = logreg_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe_logreg = accuracy_score(y_test, y_pred_rfe_logreg)\n",
        "print(f\"Logistic Regression Accuracy with RFE: {accuracy_rfe_logreg:.4f}\")\n",
        "\n",
        "# Train and evaluate Decision Tree\n",
        "tree_rfe = DecisionTreeClassifier(random_state=42)\n",
        "tree_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe_tree = tree_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe_tree = accuracy_score(y_test, y_pred_rfe_tree)\n",
        "print(f\"Decision Tree Accuracy with RFE: {accuracy_rfe_tree:.4f}\")\n"
      ],
      "metadata": {
        "id": "pc5zG2MJ5AZU",
        "outputId": "2d3397c2-00cb-4234-8065-4aff775447e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy with RFE: 1.0000\n",
            "Decision Tree Accuracy with RFE: 1.0000\n"
          ]
        }
      ]
    }
  ]
}